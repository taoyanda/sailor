{
	"global_batch_size": 128,
	"type": "gpt2",
	"hidden_size": 4096,
	"sequence_length": 2048,
	"num_layers": 32,
	"vocab_size": 50272,
	"model": "GPT.6_7B",
	"optimizer": "Adam",
	"heads": 32,
	"head_dim": 128,
	"max_position_embeddings": 2048,
	"num_all_layers": 34
}