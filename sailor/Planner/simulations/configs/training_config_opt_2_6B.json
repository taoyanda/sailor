{
	"global_batch_size": 128,
	"type": "gpt2",
	"hidden_size": 2560,
	"sequence_length": 2048,
	"num_layers": 32,
	"vocab_size": 50272,
	"model": "GPT.2_6B",
	"optimizer": "Adam",
    "heads": 32,
    "head_dim": 80,
	"max_position_embeddings": 2048,
	"num_all_layers": 34
}
